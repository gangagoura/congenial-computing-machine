{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "    <a href=\"https://cocl.us/System_ML_notebook\">\n",
    "         <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0111EN/Ad/TopAd.png\" width=\"750\" align=\"center\">\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai/\">\n",
    "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0111EN/Ad/CCLog.png\" width=\"200\" align=\"center\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SystemML and the Spark MLContext\n",
    "\n",
    "Exercise objectives:\n",
    "- Run sample SystemML code with the MLContext\n",
    "- Run the Linear Regression example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use one of the released version, use <code>%AddDeps org.apache.systemml systemml 1.2.0</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%AddDeps org.apache.systemml systemml 1.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: To use SystemML in a Spark shell with YARN, you would run this command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`./bin/spark-shell --master yarn-client --num-executors 3 --driver-memory 5G --executor-memory 5G --executor-cores 4 --jars SystemML.jar`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the <code>mlcontext</code> library and the <code>SQLContext</code> library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.sysml.api.mlcontext._\n",
    "import org.apache.spark.sql.SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val ml = new MLContext(sc)                       \n",
    "val sqlContext = new SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The <code>SparkContext</code> <code>sc</code> has been initialized for you in this notebook. If you are using the Spark shell, it is also initialized for you at the start. When you write your own Spark application, you must explicitly define your <code>SparkContext</code> variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the capability of <code>systemml</code>, let's create a DataFrame with a random 100000 rows and 1000 columns. First import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types.{StructType,StructField,DoubleType}\n",
    "import scala.util.Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the variables for the rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val numRows = 100000\n",
    "val numCols = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the <code>RDD</code> with random doubles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val data = sc.parallelize(0 to numRows-1).map{ _ => Row.fromSeq(Seq.fill(numCols)(Random.nextDouble)) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val schema = StructType((0 to numCols-1).map { i => StructField(\"C\" + i, DoubleType, true) } )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val df = sqlContext.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating helper methods\n",
    "Create some helper methods. The <code>systemml</code> output data is encapsulated in an <code>MLOutput</code> object. The <code>getScalar()</code> method extracts a scalar value from a DataFrame returned by MLOutput. The <code>getScalarDouble()</code> method returns such a value as a Double, and the <code>getScalarInt()</code> method returns such a value as an Int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getScalar(outputs: MLResults, symbol: String): Any = outputs.getDataFrame(symbol).first()(1)\n",
    "def getScalarDouble(outputs: MLResults, symbol: String): Double = getScalar(outputs, symbol).asInstanceOf[Double]\n",
    "def getScalarInt(outputs: MLResults, symbol: String): Int = getScalarDouble(outputs, symbol).toInt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary-block matrix representation\n",
    "<code>SystemML</code> is optimized to operate on a binary-block format for matrix representation. For large datasets, conversion from DataFrame to binary-block can require a significant quantity of time. Explicit DataFrame to binary-block conversion allows algorithm performance to be measured separately from data conversion time.\n",
    "\n",
    "The <code>SystemML</code> binary-block matrix representation can be thought of as a two-dimensional array of blocks, where each block consists of a number of rows and columns. In this example, we specify a matrix consisting of blocks of size 1000x1000. The experimental <code>dataFrameToBinaryBlock()</code> method of <code>RDDConverterUtils</code> is used to convert the DataFrame <code>df</code> to a <code>SystemML</code> binary-block matrix, which is represented by the datatype <code>JavaPairRDD[MatrixIndexes, MatrixBlock]</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required libraries and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.sysml.runtime.instructions.spark.utils.RDDConverterUtils\n",
    "import org.apache.sysml.runtime.matrix.MatrixCharacteristics;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val numRowsPerBlock = 10\n",
    "val numColsPerBlock = 10\n",
    "val mc = new MatrixCharacteristics(numRows, numCols, numRowsPerBlock, numColsPerBlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the conversion to binary-block matrix using the <code>RDDConverterUtils</code> class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val sysMlMatrix = RDDConverterUtils.dataFrameToBinaryBlock(sc, df, mc, false, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the `LinearDataGenerator` library to assist with generating some raw data. Import the `sqlContext.implicits._` to convert RDD to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.util.{LinearDataGenerator, MLUtils}\n",
    "import sqlContext.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use LinearDataGenerator to create test data of 10000 rows with 1000 columns and convert that into a DataFrame to be used for Spark ML and the SystemML linear regression algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val numRows = 1000\n",
    "val numCols = 100\n",
    "val rawData = {\n",
    "  val tmp = LinearDataGenerator.generateLinearRDD(sc, numRows, numCols, 1).toDF()\n",
    "  MLUtils.convertVectorColumnsToML(tmp, \"features\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repartition into a more parallelism-friendly number of partitions and cache it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val data = rawData.repartition(64).cache()\n",
    "println(data.getClass.getName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Spark Linear Regression library. We'll first train a model using the Spark ML linear regression algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val maxIters = 100\n",
    "val reg = 0\n",
    "val elasticNetParam = 0  // L2 reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val lr = new LinearRegression().setMaxIter(maxIters).setRegParam(reg).setElasticNetParam(elasticNetParam)\n",
    "val start = System.currentTimeMillis()\n",
    "val model = lr.fit(data)\n",
    "val trainingTime = (System.currentTimeMillis() - start).toDouble / 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the model over the training set and gather some metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val trainingSummary = model.summary\n",
    "val r2 = trainingSummary.r2\n",
    "val iters = trainingSummary.totalIterations\n",
    "val trainingTimePerIter = trainingTime / iters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(s\"R2: ${r2}\")\n",
    "println(s\"Iterations: ${iters}\")\n",
    "println(s\"Training time per iter: ${trainingTimePerIter} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the training time it took per iteration. We'll compare that to SystemML's training time below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SystemML kernels. \n",
    "This is where the magic of SystemML takes place. The <code>linearReg</code> fixed String variable is set to a linear regression algorithm written in DML, SystemMLâ€™s Declarative Machine Learning language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val linearReg =\n",
    "\"\"\"\n",
    "#\n",
    "# THIS SCRIPT SOLVES LINEAR REGRESSION USING THE CONJUGATE GRADIENT ALGORITHM\n",
    "#\n",
    "# INPUT PARAMETERS:\n",
    "# --------------------------------------------------------------------------------------------\n",
    "# NAME  TYPE   DEFAULT  MEANING\n",
    "# --------------------------------------------------------------------------------------------\n",
    "# X     String  ---     Matrix X of feature vectors\n",
    "# Y     String  ---     1-column Matrix Y of response values\n",
    "# icpt  Int      0      Intercept presence, shifting and rescaling the columns of X:\n",
    "#                       0 = no intercept, no shifting, no rescaling;\n",
    "#                       1 = add intercept, but neither shift nor rescale X;\n",
    "#                       2 = add intercept, shift & rescale X columns to mean = 0, variance = 1\n",
    "# reg   Double 0.000001 Regularization constant (lambda) for L2-regularization; set to nonzero\n",
    "#                       for highly dependend/sparse/numerous features\n",
    "# tol   Double 0.000001 Tolerance (epsilon); conjugate graduent procedure terminates early if\n",
    "#                       L2 norm of the beta-residual is less than tolerance * its initial norm\n",
    "# maxi  Int      0      Maximum number of conjugate gradient iterations, 0 = no maximum\n",
    "# --------------------------------------------------------------------------------------------\n",
    "#\n",
    "# OUTPUT:\n",
    "# B Estimated regression parameters (the betas) to store\n",
    "#\n",
    "# Note: Matrix of regression parameters (the betas) and its size depend on icpt input value:\n",
    "#         OUTPUT SIZE:   OUTPUT CONTENTS:                HOW TO PREDICT Y FROM X AND B:\n",
    "# icpt=0: ncol(X)   x 1  Betas for X only                Y ~ X %*% B[1:ncol(X), 1], or just X %*% B\n",
    "# icpt=1: ncol(X)+1 x 1  Betas for X and intercept       Y ~ X %*% B[1:ncol(X), 1] + B[ncol(X)+1, 1]\n",
    "# icpt=2: ncol(X)+1 x 2  Col.1: betas for X & intercept  Y ~ X %*% B[1:ncol(X), 1] + B[ncol(X)+1, 1]\n",
    "#                        Col.2: betas for shifted/rescaled X and intercept\n",
    "#\n",
    "\n",
    "fileX = \"\";\n",
    "fileY = \"\";\n",
    "fileB = \"\";\n",
    "\n",
    "intercept_status = ifdef ($icpt, 0);     # $icpt=0;\n",
    "tolerance = ifdef ($tol, 0.000001);      # $tol=0.000001;\n",
    "max_iteration = ifdef ($maxi, 0);        # $maxi=0;\n",
    "regularization = ifdef ($reg, 0.000001); # $reg=0.000001;\n",
    "\n",
    "X = read (fileX);\n",
    "y = read (fileY);\n",
    "\n",
    "n = nrow (X);\n",
    "m = ncol (X);\n",
    "ones_n = matrix (1, rows = n, cols = 1);\n",
    "zero_cell = matrix (0, rows = 1, cols = 1);\n",
    "\n",
    "# Introduce the intercept, shift and rescale the columns of X if needed\n",
    "\n",
    "m_ext = m;\n",
    "if (intercept_status == 1 | intercept_status == 2)  # add the intercept column\n",
    "{\n",
    "    X = append (X, ones_n);\n",
    "    m_ext = ncol (X);\n",
    "}\n",
    "\n",
    "scale_lambda = matrix (1, rows = m_ext, cols = 1);\n",
    "if (intercept_status == 1 | intercept_status == 2)\n",
    "{\n",
    "    scale_lambda [m_ext, 1] = 0;\n",
    "}\n",
    "\n",
    "if (intercept_status == 2)  # scale-&-shift X columns to mean 0, variance 1\n",
    "{                           # Important assumption: X [, m_ext] = ones_n\n",
    "    avg_X_cols = t(colSums(X)) / n;\n",
    "    var_X_cols = (t(colSums (X ^ 2)) - n * (avg_X_cols ^ 2)) / (n - 1);\n",
    "    is_unsafe = ppred (var_X_cols, 0.0, \"<=\");\n",
    "    scale_X = 1.0 / sqrt (var_X_cols * (1 - is_unsafe) + is_unsafe);\n",
    "    scale_X [m_ext, 1] = 1;\n",
    "    shift_X = - avg_X_cols * scale_X;\n",
    "    shift_X [m_ext, 1] = 0;\n",
    "} else {\n",
    "    scale_X = matrix (1, rows = m_ext, cols = 1);\n",
    "    shift_X = matrix (0, rows = m_ext, cols = 1);\n",
    "}\n",
    "\n",
    "# Henceforth, if intercept_status == 2, we use \"X %*% (SHIFT/SCALE TRANSFORM)\"\n",
    "# instead of \"X\".  However, in order to preserve the sparsity of X,\n",
    "# we apply the transform associatively to some other part of the expression\n",
    "# in which it occurs.  To avoid materializing a large matrix, we rewrite it:\n",
    "#\n",
    "# ssX_A  = (SHIFT/SCALE TRANSFORM) %*% A    --- is rewritten as:\n",
    "# ssX_A  = diag (scale_X) %*% A;\n",
    "# ssX_A [m_ext, ] = ssX_A [m_ext, ] + t(shift_X) %*% A;\n",
    "#\n",
    "# tssX_A = t(SHIFT/SCALE TRANSFORM) %*% A   --- is rewritten as:\n",
    "# tssX_A = diag (scale_X) %*% A + shift_X %*% A [m_ext, ];\n",
    "\n",
    "lambda = scale_lambda * regularization;\n",
    "beta_unscaled = matrix (0, rows = m_ext, cols = 1);\n",
    "\n",
    "if (max_iteration == 0) {\n",
    "    max_iteration = m_ext;\n",
    "}\n",
    "i = 0;\n",
    "\n",
    "# BEGIN THE CONJUGATE GRADIENT ALGORITHM\n",
    "r = - t(X) %*% y;\n",
    "\n",
    "if (intercept_status == 2) {\n",
    "    r = scale_X * r + shift_X %*% r [m_ext, ];\n",
    "}\n",
    "\n",
    "p = - r;\n",
    "norm_r2 = sum (r ^ 2);\n",
    "norm_r2_initial = norm_r2;\n",
    "norm_r2_target = norm_r2_initial * tolerance ^ 2;\n",
    "\n",
    "while (i < max_iteration & norm_r2 > norm_r2_target)\n",
    "{\n",
    "    if (intercept_status == 2) {\n",
    "        ssX_p = scale_X * p;\n",
    "        ssX_p [m_ext, ] = ssX_p [m_ext, ] + t(shift_X) %*% p;\n",
    "    } else {\n",
    "        ssX_p = p;\n",
    "    }\n",
    "\n",
    "    q = t(X) %*% (X %*% ssX_p);\n",
    "\n",
    "    if (intercept_status == 2) {\n",
    "        q = scale_X * q + shift_X %*% q [m_ext, ];\n",
    "    }\n",
    "\n",
    "    q = q + lambda * p;\n",
    "    a = norm_r2 / sum (p * q);\n",
    "    beta_unscaled = beta_unscaled + a * p;\n",
    "    r = r + a * q;\n",
    "    old_norm_r2 = norm_r2;\n",
    "    norm_r2 = sum (r ^ 2);\n",
    "    p = -r + (norm_r2 / old_norm_r2) * p;\n",
    "    i = i + 1;\n",
    "}\n",
    "# END THE CONJUGATE GRADIENT ALGORITHM\n",
    "\n",
    "if (intercept_status == 2) {\n",
    "    beta = scale_X * beta_unscaled;\n",
    "    beta [m_ext, ] = beta [m_ext, ] + t(shift_X) %*% beta_unscaled;\n",
    "} else {\n",
    "    beta = beta_unscaled;\n",
    "}\n",
    "\n",
    "# Output statistics\n",
    "avg_tot = sum (y) / n;\n",
    "ss_tot = sum (y ^ 2);\n",
    "ss_avg_tot = ss_tot - n * avg_tot ^ 2;\n",
    "var_tot = ss_avg_tot / (n - 1);\n",
    "y_residual = y - X %*% beta;\n",
    "avg_res = sum (y_residual) / n;\n",
    "ss_res = sum (y_residual ^ 2);\n",
    "ss_avg_res = ss_res - n * avg_res ^ 2;\n",
    "\n",
    "R2_temp = 1 - ss_res / ss_avg_tot\n",
    "R2 = matrix(R2_temp, rows=1, cols=1)\n",
    "write(R2, \"\")\n",
    "\n",
    "totalIters = matrix(i, rows=1, cols=1)\n",
    "write(totalIters, \"\")\n",
    "\n",
    "# Prepare the output matrix\n",
    "if (intercept_status == 2) {\n",
    "    beta_out = append (beta, beta_unscaled);\n",
    "} else {\n",
    "    beta_out = beta;\n",
    "}\n",
    "\n",
    "write (beta_out, fileB);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data to proper format. Remember that we wanted to convert to a binary-block matrix representation prior to the actual execution of the algorithm. This would allow us to accurately capture the algorithm performance without the data conversion time affecting the measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val mcX = new MatrixCharacteristics(numRows, numCols, 1000, 1000)\n",
    "val mcY = new MatrixCharacteristics(numRows, 1, 1000, 1000)\n",
    "val X = RDDConverterUtils.dataFrameToBinaryBlock(sc, data.select(\"features\"), mcX, false, true)\n",
    "val y = RDDConverterUtils.dataFrameToBinaryBlock(sc, data.select(\"label\"), mcY, false, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(mcX.getClass())\n",
    "println(mcY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val X2 = X.cache()\n",
    "val y2 = y.cache()\n",
    "val cnt1 = X2.count()\n",
    "val cnt2 = y2.count() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can train our model using the <code>SystemML</code> linear regression algorithm. We register the features matrix X and the label matrix y as inputs. We register the <code>beta_out</code> matrix, <code>R2</code>, and <code>totalIters</code> as outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml.resetConfig()\n",
    "val Xmat = new MatrixMetadata(numRows, numCols)\n",
    "val ymat = new MatrixMetadata(numRows, 1)\n",
    "val X3 = MLContextConversionUtil.matrixObjectTo2DDoubleArray(MLContextConversionUtil.binaryBlocksToMatrixObject(X, Xmat))\n",
    "val y3 = MLContextConversionUtil.matrixObjectTo2DDoubleArray(MLContextConversionUtil.binaryBlocksToMatrixObject(y, ymat))\n",
    "val dmlscript = ScriptFactory.dml(linearReg).in(\"X\", X3).in(\"y\", y3).out(\"beta_out\", \"R2\", \"totalIters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the script. Here is where you passed in that <code>linearReg</code> variable that was defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val start = System.currentTimeMillis()\n",
    "val outputs = ml.execute(dmlscript)\n",
    "val trainingTime = (System.currentTimeMillis() - start).toDouble / 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get outputs. Note that we are using the helper methods that we defined earlier. The helper methods return Double and Int values from output generated by the <code>mlcontext</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val B = outputs.getDataFrame(\"beta_out\")\n",
    "val r2 = getScalarDouble(outputs, \"R2\")\n",
    "val iters = getScalarInt(outputs, \"totalIters\")\n",
    "val trainingTimePerIter = trainingTime / iters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(s\"R2: ${r2}\")\n",
    "println(s\"Iterations: ${iters}\")\n",
    "println(s\"Training time per iter: ${trainingTimePerIter} seconds\")\n",
    "B.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the training time here. How does this compare with the Spark ML linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "<h2>Get IBM Watson Studio free of charge!</h2>\n",
    "    <p><a href=\"https://cocl.us/System_ML_notebook\"><img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0111EN/Ad/BottomAd.png\" width=\"750\" align=\"center\"></a></p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
